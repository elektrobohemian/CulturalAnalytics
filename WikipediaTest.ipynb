{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Unstructured Data\n",
    "\n",
    "In this tutorial you will learn to:\n",
    "\n",
    "* [web/screen scrape](https://en.wikipedia.org/wiki/Web_scraping) relatively unstructured data from the Wikipedia\n",
    "* transform unstructured data into tabular data to facilitate processing with Python\n",
    "* create graph data from your data to visualize your data as networks\n",
    "* export Python-created data to use it with JavaScript visualization libraries such as [D3.js](https://d3js.org/)\n",
    "\n",
    "Would you should already know:\n",
    "\n",
    "* a little Python 3\n",
    "* some minor HTML\n",
    "* some JavaScript if you want to understand the web-based visualization at the end of the tutorial\n",
    "\n",
    "This notebook assumes that you are using [Anaconda](https://www.anaconda.com/download/) as your Python 3 distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* DC und Marvel gegen√ºberstellen,\n",
    "* WEbsite entsprechend anpassen\n",
    "* auf Partnerships eingehen\n",
    "* Konzepte in den Abilities bzw. Sentiments extrahieren\n",
    "* entsprechende Wikipedia-Artikel auf Sentiments hin analysieren\n",
    "* Korrelationen heraussuchen (einsame Superhelden mit negativen Artikeln in der Wikipedia?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Your Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The %... is an iPython thing, and is not part of the Python language.\n",
    "# In this case we're just telling the plotting library to draw things on\n",
    "# the notebook, instead of on a separate window.\n",
    "%matplotlib inline\n",
    "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
    "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict # provides the ordered dictionary\n",
    "import re # for regular expressions used below\n",
    "import urllib # to read from URLs\n",
    "import networkx as nx # network analysis\n",
    "import itertools\n",
    "from datetime import datetime # for time measurement\n",
    "import sys\n",
    "import os\n",
    "import unicodedata as ucd\n",
    "\n",
    "# utility method that displays a given text and the current time\n",
    "def printLog(text):\n",
    "    now=str(datetime.now())\n",
    "    print(\"[\"+now+\"]\\t\"+text)\n",
    "    # forces to output the result of the print command immediately, see: http://stackoverflow.com/questions/230751/how-to-flush-output-of-python-print\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to work with superheroes of the DC Comics or Marvel universe. However, only one universe can be processed at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-01 11:37:37.495809]\tAnalysing Marvel universe...\n"
     ]
    }
   ],
   "source": [
    "isMarvel=True\n",
    "\n",
    "if isMarvel:\n",
    "    # Marvel\n",
    "    imageDownloadPath=\"./web/force/img.marvel_chars/\"\n",
    "    localRelativeImagePath=\"img.marvel_chars/\"\n",
    "    csvFilePath=\"marvel_superheroes.csv\"\n",
    "    pathPrefix=\"marvel\"\n",
    "    printLog(\"Analysing Marvel universe...\")\n",
    "else:\n",
    "    # DC Comics\n",
    "    imageDownloadPath=\"./web/force/img.dc_chars/\"\n",
    "    localRelativeImagePath=\"img.dc_chars/\"\n",
    "    csvFilePath=\"dc_superheroes.csv\"\n",
    "    pathPrefix=\"dc\"\n",
    "    printLog(\"Analysing DC universe...\")\n",
    "\n",
    "# if the needed paths do not exist, create them\n",
    "if not os.path.exists(imageDownloadPath):\n",
    "    os.mkdir(imageDownloadPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen-Scraping\n",
    "Anaconda comes with _Beautiful Soup_, a library for screen-scraping [documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). The main idea is to download a HTML page from the Wikipedia, parse its contents, and save it in a structed way. This functionality is encapsulated in the function _extractData()_ (see below).\n",
    "\n",
    "The relevant data is hidden in a table with the CSS class _infobox_. The _th_ tags serve as separators for the following information:\n",
    "* <th scope=\"row\" style=\"width: 40%;\">Alter ego</th>\n",
    "* <th scope=\"row\" style=\"width: 40%;\">Team affiliations</th>\n",
    "* <th scope=\"row\" style=\"width: 40%;\">Partnerships</th>\n",
    "* <th scope=\"row\" style=\"width: 40%;\">Notable aliases</th>\n",
    "* <th scope=\"row\" style=\"width: 40%;\">Abilities</th>\n",
    "\n",
    "The function also relies on [_regular expressions_](https://docs.python.org/2/howto/regex.html) - if you haven't heard of them before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(myDoc): # myDoc is the HTML document to be parsed\n",
    "    # initialize the screen-scraper\n",
    "    soup = BeautifulSoup(myDoc, 'html.parser')\n",
    "    # during an inspection of the Wikipedia HTML, we discovered that table elements with the CSS class infobox have to be parsed\n",
    "    tables=soup.find_all('table',class_=\"infobox\")\n",
    "    # if there is no infobox, ignore the page\n",
    "    if len(tables)>=1:\n",
    "        # take the first infobox\n",
    "        infoboxTable=tables[0]\n",
    "        # try to get the name of the character\n",
    "        if infoboxTable.tr.th is not None:\n",
    "            name=infoboxTable.tr.th.get_text()\n",
    "        else:\n",
    "            #if it is not part of the infobox take it from the HTML page's title and strip the Wikipedia reference\n",
    "            name=soup.title.get_text().replace(\" - Wikipedia, the free encyclopedia\",\"\")\n",
    "        # prepare variables for the data to be extracted\n",
    "        abilityRow=None\n",
    "        partnerRow=None\n",
    "        firstAppearance=\"2020\" # dummy entry in case the \"first appearance\" entry is missing\n",
    "        abilities=[]\n",
    "        partnerships=[]\n",
    "        \n",
    "        # save the provided image for later processing\n",
    "        if infoboxTable.find('img'):\n",
    "            articleImage=infoboxTable.find('img')[\"src\"]\n",
    "        else:\n",
    "            articleImage=None\n",
    "\n",
    "        p = re.compile('\\d\\d\\d\\d') # we expect the year of first appearance to have four digits\n",
    "        \n",
    "        rows=infoboxTable.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            if row.th is not None and row.th.string == \"First appearance\":\n",
    "                rawText=row.td.get_text()\n",
    "                m = p.search(rawText)\n",
    "                # deal with missing years of first appearance\n",
    "                if m:\n",
    "                    firstAppearance=m.group()\n",
    "            elif row.th is not None and row.th.string == \"Abilities\":\n",
    "                abilityRow=row\n",
    "            elif row.th is not None and row.th.string == \"Partnerships\":\n",
    "                partnerRow=row\n",
    "\n",
    "        # as not every page has a \"complete\" infobox, we have to check the presence of abilities and partnerships\n",
    "        if abilityRow is not None:\n",
    "            li=abilityRow.find_all(\"li\")\n",
    "            for l in li:\n",
    "                # check if an anchor (a link to a website) is contained\n",
    "                if l.a is not None:\n",
    "                    # if so, discard the link and keep the plain text\n",
    "                    l.a.unwrap()\n",
    "                abilities.append(l.get_text().strip())\n",
    "        # a fix for abilities without the ul/li structure\n",
    "            if len(li)==0:\n",
    "                rawStr=str(abilityRow.td)\n",
    "                rawStr=rawStr.replace(\"<td>\",\"\").replace(\"</td>\",\"\").replace(\"\\n\",\"\").strip()\n",
    "                # abilities are sometimes separated by  <br/>, a comma, a period, or \"and\"\n",
    "                rawStr=rawStr.replace(\"<br/>\",\";\")\n",
    "                rawStr=rawStr.replace(\",\",\";\")\n",
    "                rawStr=rawStr.replace(\".\",\";\")\n",
    "                # we have to treat 'and' in a special way as it may be contained as a substring in \"normal\" words\n",
    "                #rawStr=rawStr.replace(\"and\",\";\")\n",
    "                rawTokens=rawStr.split(\" and \")\n",
    "                rawStr=\";\".join(rawTokens)\n",
    "                # remove all other HTML tags\n",
    "                p2 = re.compile(r'<.*?>')\n",
    "                rawStr=p2.sub(' ', rawStr)\n",
    "                # clean whitespaces surrounding the string\n",
    "                rawStr.strip()\n",
    "                rawTokens=rawStr.split(\";\")\n",
    "                for t in rawTokens:\n",
    "                    t=t.strip()\n",
    "                    # we have to ignore additional noise such as the following \"abilities\"\n",
    "                    if t.lower()==\"see below\":\n",
    "                        pass\n",
    "                    elif t.lower()==\"various\":\n",
    "                        pass\n",
    "                    elif t.lower()==\"varies\":\n",
    "                        pass\n",
    "                    elif t.lower()==\"none\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        if str(t):\n",
    "                            abilities.append(str(t))\n",
    "\n",
    "        if partnerRow is not None:\n",
    "            partners=partnerRow.find_all(\"a\")\n",
    "            for p in partners:\n",
    "                if p.string is not None: # some Marvel pages \"suffer\" from empty <a>\n",
    "                    partnerships.append(p.string)\n",
    "\n",
    "        \n",
    "        \n",
    "        result=dict()\n",
    "        result[\"name\"]=name\n",
    "        result[\"firstAppearance\"]=int(firstAppearance)\n",
    "        result[\"abilities\"]=abilities\n",
    "        result[\"abilitiesCount\"]=len(abilities)\n",
    "        result[\"partnerships\"]=partnerships\n",
    "        result[\"partnershipsCount\"]=len(partnerships)\n",
    "        if articleImage:\n",
    "            result[\"imageURL\"]=\"https:\"+str(articleImage)\n",
    "        else:\n",
    "            result[\"imageURL\"]=\"https://None\"\n",
    "        #print abilities\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Function\n",
    "In order to find out whether the function works, we call it with two superheroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Juggernaut', 'firstAppearance': 1965, 'abilities': ['Superhuman strength', 'stamina', 'durability', 'Invulnerability via mystical force field', 'Unstoppable momentum', 'Immunity to mental attacks via his helmet'], 'abilitiesCount': 6, 'partnerships': ['Black Tom Cassidy'], 'partnershipsCount': 1, 'imageURL': 'https://upload.wikimedia.org/wikipedia/en/thumb/4/44/Juggernaut2.PNG/250px-Juggernaut2.PNG'}\n",
      "\n",
      " NEXT SUPERHERO\n",
      "\n",
      "{'name': 'BatmanBruce Wayne', 'firstAppearance': 1939, 'abilities': ['Genius intellect', 'Expert detective', 'Skilled martial artist and hand-to-hand combatant', 'Master tactician, strategist, and field commander', 'Utilizing high-tech equipment'], 'abilitiesCount': 5, 'partnerships': ['Robin', 'Alfred Pennyworth', 'James Gordon', 'Harvey Dent', 'Catwoman', 'Batgirl', 'Superman', 'Wonder Woman'], 'partnershipsCount': 8, 'imageURL': 'https://upload.wikimedia.org/wikipedia/en/c/c7/Batman_Infobox.jpg'}\n"
     ]
    }
   ],
   "source": [
    "html_doc=urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Juggernaut_(comics)\")\n",
    "r=extractData(html_doc)\n",
    "print(r)\n",
    "\n",
    "print(\"\\n NEXT SUPERHERO\\n\")\n",
    "\n",
    "html_doc=urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Batman\")\n",
    "r=extractData(html_doc)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the reality is more complex but the Wikipedia provides lists of superheroes of the different universes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryPages=[]\n",
    "if not isMarvel:\n",
    "    # DC superheroes\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:DC_Comics_superheroes&pageuntil=Dragon+King%0ADragon+King+%28DC+Comics%29#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:DC_Comics_superheroes&pagefrom=Krypto#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:DC_Comics_superheroes&pagefrom=Robin%0ARobin+%28comics%29#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:DC_Comics_superheroes&pagefrom=XS+%28comics%29#mw-pages\")\n",
    "    # general DC characters\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:DC_Comics_characters&pageuntil=Smoak%2C+Felicity%0AFelicity+Smoak#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:DC_Comics_characters&pagefrom=Smoak%2C+Felicity%0AFelicity+Smoak#mw-pages\")\n",
    "else:\n",
    "    # Marvel superheroes\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:Marvel_Comics_superheroes&pageuntil=Dazzler#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:Marvel_Comics_superheroes&pagefrom=Dazzler#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:Marvel_Comics_superheroes&pagefrom=Jameson%0AJohn+Jameson+%28comics%29#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:Marvel_Comics_superheroes&pagefrom=Prime+%28Comics%29%0APrime+%28comics%29#mw-pages\")\n",
    "    categoryPages.append(\"https://en.wikipedia.org/w/index.php?title=Category:Marvel_Comics_superheroes&pagefrom=Talon+%28Marvel+Comics%29#mw-pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function expects Wikipedia category pages as provided above and retrieves the URLs of relevant superhero Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-01 11:37:39.037237]\tFetching superhero URLs...\n",
      "[2021-03-01 11:37:42.710529]\tProcessing completed in 0:00:03.673254.\n"
     ]
    }
   ],
   "source": [
    "def getSuperheroes(myCategories,myDictionary):\n",
    "    for c in myCategories:\n",
    "    # ignore <a class=\"mw-redirect\"> as they redirect into articles, \n",
    "    # to be precise, we only consider <a> tags without a class atrribute \n",
    "    # because we are only interested in superheroes with distinct wikipedia articles\n",
    "        items=c.ul.find_all(\"li\")\n",
    "        for item in items:\n",
    "            if item.a.get(\"class\") is None:\n",
    "                myDictionary[item.a[\"title\"]]=item.a[\"href\"]\n",
    "\n",
    "\n",
    "startTime=datetime.now()\n",
    "printLog(\"Fetching superhero URLs...\")\n",
    "\n",
    "superheroes=OrderedDict() # we use an ordered dict here because we want to preserve the order items were added\n",
    "for catPage in categoryPages:\n",
    "    html_doc=urllib.request.urlopen(catPage)\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    categories=soup.select(\"#mw-pages\")[0].select(\".mw-category-group\")\n",
    "    getSuperheroes(categories,superheroes) # we pass the superheroes dictionary on to merge the heroes extracted from all pages\n",
    "\n",
    "duration=datetime.now()-startTime\n",
    "printLog(\"Processing completed in \"+str(duration)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of URLs, we can extract the data for each superhero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-01 11:37:42.719492]\tProcessing 659 Wikipedia pages. This will take a while...\n",
      "\thttps://en.wikipedia.org/wiki/Citizen_V does not contain an infobox. Processing aborted.\n",
      "\thttps://en.wikipedia.org/wiki/Red_Hulk does not contain an infobox. Processing aborted.\n",
      "\thttps://en.wikipedia.org/wiki/Thunderstrike_(comics) does not contain an infobox. Processing aborted.\n",
      "[2021-03-01 11:40:36.867641]\tProcessing completed in 0:02:54.147161.\n"
     ]
    }
   ],
   "source": [
    "wikiBaseURL=\"https://en.wikipedia.org\"\n",
    "heroFeats=[]\n",
    "i=0;\n",
    "countItems=len(superheroes)\n",
    "\n",
    "printLog(\"Processing \"+str(countItems)+\" Wikipedia pages. This will take a while...\")\n",
    "\n",
    "startTime=datetime.now()\n",
    "\n",
    "\n",
    "for key, value in superheroes.items():\n",
    "    i=i+1\n",
    "    wikiURL=wikiBaseURL+value\n",
    "    #print \"Processing \"+wikiURL+\" ; \"+str(i)+\" of \"+str(countItems)\n",
    "    html_doc=urllib.request.urlopen(wikiURL)\n",
    "    r=extractData(html_doc)\n",
    "    if r:\n",
    "        r[\"url\"]=wikiURL\n",
    "        heroFeats.append(r)\n",
    "    else:\n",
    "        print(\"\\t\"+wikiURL+\" does not contain an infobox. Processing aborted.\")\n",
    "\n",
    "duration=datetime.now()-startTime\n",
    "printLog(\"Processing completed in \"+str(duration)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we will download the character images if there are some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-01 11:40:36.878885]\tDownloading images. This will take a while...\n"
     ]
    }
   ],
   "source": [
    "printLog(\"Downloading images. This will take a while...\")\n",
    "\n",
    "startTime=datetime.now()\n",
    "# downloading images\n",
    "\n",
    "if not os.path.exists(imageDownloadPath):\n",
    "    os.makedirs(imageDownloadPath)\n",
    "\n",
    "for i,hero in enumerate(heroFeats):\n",
    "    hero[\"localFilePath\"]=\"N/A\" # caveat of JSON http://stackoverflow.com/questions/13715891/d3-json-uncaught-typeerror-cannot-read-property-children-of-undefined\n",
    "    iURL=hero[\"imageURL\"]\n",
    "    if not iURL==\"https://None\":\n",
    "        tokens=iURL.split(\".\")\n",
    "        suffix=tokens[-1]\n",
    "        localFilePath=imageDownloadPath+str(i)+\".\"+suffix\n",
    "        # debug\n",
    "        #print(\"Downloading to: \"+localFilePath)\n",
    "        hero[\"localFilePath\"]=localRelativeImagePath+str(i)+\".\"+suffix\n",
    "        try:\n",
    "            urllib.request.urlretrieve(iURL,localFilePath)\n",
    "        except IOError: # if we would catch all other exception, we would have a hard time to stop the kernel at all\n",
    "            print(\"Downloading error while accessing: \"+hero[\"imageURL\"]+ \"and/or saving in: \"+localFilePath)\n",
    "            #print(IOError)\n",
    "            #break\n",
    "duration=datetime.now()-startTime\n",
    "printLog(\"Downloading completed in \"+str(duration)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Raw Data into Tables\n",
    "\n",
    "For further processing relying on Python's typical data science libraries, it is handy to save the data in a Pandas [DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), a tabular data structure with built-in support for various analytical tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows=[]\n",
    "columns=[\"Name\",\"Year\",\"Decade\",\"Abilities\",\"AbilitiesCount\",\"Partnerships\",\"PartnershipsCount\",\"URL\",\"ImageURL\",\"localFilePath\"]\n",
    "\n",
    "# take each hero and save attributes in a data frame\n",
    "for hero in heroFeats:\n",
    "    #print(hero[\"name\"])\n",
    "    rowx=[]\n",
    "    rowx.append(hero[\"name\"])\n",
    "    firstAp=hero[\"firstAppearance\"]\n",
    "    rowx.append(firstAp)\n",
    "    # calculate the decade\n",
    "    if firstAp<2000:\n",
    "        rowx.append(1900+int((hero[\"firstAppearance\"]-1900)/10)*10)\n",
    "    else:\n",
    "        rowx.append(2000+int((hero[\"firstAppearance\"]-2000)/10)*10)\n",
    "    rowx.append(\",\".join(hero[\"abilities\"]))\n",
    "    rowx.append(hero[\"abilitiesCount\"])\n",
    "    rowx.append(\",\".join(hero[\"partnerships\"]))\n",
    "    rowx.append(hero[\"partnershipsCount\"])\n",
    "    rowx.append(hero[\"url\"])\n",
    "    rowx.append(hero[\"imageURL\"])\n",
    "    rowx.append(hero[\"localFilePath\"])\n",
    "    rows.append(rowx)\n",
    "\n",
    "df=pd.DataFrame(rows,columns=columns)\n",
    "df.head() # display the top rows of the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the table is not ready for processing. For instance, the superheroe's name are using lower and upper case letters which would complicate later comparisons. There are also encoding issues. Hence, we normalize textual date before we continue with the analysis.\n",
    "\n",
    "To simplify the code, we rely on [lambda functions](https://www.w3schools.com/python/python_lambda.asp), anonymous functions that get applied on most attributes of the data frame. In order to be non-destructive, we save the results in separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid spelling issues, we capitalize all superheroes' names\n",
    "# lambda Konzept erl√§utern\n",
    "try:\n",
    "    df['NameCaps'] = df.Name.apply(lambda x: x.upper())\n",
    "except(UnicodeEncodeError):\n",
    "    df['NameCaps'] = df.Name.apply(lambda x: ucd.normalize('NFKD', unicode(x)).encode('ascii','ignore').upper())\n",
    "#    utf8str=row[13].decode('utf-8')\n",
    "#    str2=ucd.normalize('NFKD', utf8str).encode('ascii','ignore')\n",
    "try:\n",
    "    df['PartnershipsCaps'] = df.Partnerships.astype(str).apply(lambda x: x.upper())\n",
    "except(UnicodeEncodeError):\n",
    "    df['PartnershipsCaps'] = df.Partnerships.apply(lambda x: ucd.normalize('NFKD', unicode(x)).encode('ascii','ignore').upper())\n",
    "try:\n",
    "    df['AbilitiesCaps'] = df.Abilities.astype(str).apply(lambda x: x.upper())\n",
    "except(UnicodeEncodeError):\n",
    "    df['AbilitiesCaps'] = df.Abilities.apply(lambda x: ucd.normalize('NFKD', unicode(x)).encode('ascii','ignore').upper())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the results as a CSV file. However, Excel, a database, or many more other formats would also be possible. This is also a good place to take a rest and continue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file without the index columns of the data frame and separate each field with a tabstop\n",
    "df.to_csv(csvFilePath,index=False,header=True,encoding='utf-8', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing the Analysis\n",
    "If you have shutdown the notebook before, you can read in the data again with [read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(csvFilePath,encoding='utf-8', sep=\"\\t\",dtype={\"Partnerships\":'str'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test wheter all superhero names are unique - a valid hypothesis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countNames=df.NameCaps.count()\n",
    "uniqueNames=len(df.NameCaps.unique())\n",
    "\n",
    "print(\"Total number of names: %d, Unique number of names: %d\" % (countNames,uniqueNames))\n",
    "if countNames!=uniqueNames:\n",
    "    print(\"Attention! Names are not unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the names are not unique.\n",
    "To drop the duplicates, we apply a rather simple strategy: we keep the oldest, i.e., the first appearing, superheroes using pandas utility function [drop_duplicates()](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.drop_duplicates.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='NameCaps',keep='first',inplace=True)\n",
    "\n",
    "countNames=df.NameCaps.count()\n",
    "uniqueNames=len(df.NameCaps.unique())\n",
    "\n",
    "print(\"Total number of names: %d, Unique number of names: %d\" % (countNames,uniqueNames))\n",
    "if countNames!=uniqueNames:\n",
    "    print(\"Attention! Names are not unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we made sure that every superhero appears only once, we will try to find out if there are \"lone wolf\" characters. These heros are copied into a new data frame.\n",
    "Further selections are made for demonstration purposes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"lonely\" superheroes are created from the original data frame selecting each row where the column \"Partnerships\" is Null (not set)\n",
    "df2=df[df[\"Partnerships\"].isnull()]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"sociable\" superheroes\n",
    "df3=df[df[\"Partnerships\"].notnull()]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the oldest superhero(ine)\n",
    "df[df['Year']==df['Year'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the superhero with the maximal number of abilities\n",
    "df[df['AbilitiesCount']==df['AbilitiesCount'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select superheroes that first appeared during 1939 and 1945\n",
    "df[(df.Year>=1939) & (df.Year<1945)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the not so gifted superheroes with only one ability\n",
    "df[df['AbilitiesCount']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the last selection, we see that the abilities column is not in [first normal form](https://en.wikipedia.org/wiki/First_normal_form) and the abilities count is not correct for every superhero. Hence, we should consider to split the abilities column and update abilitiesCount accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "If we want to get a general \"feeling\" about our data frame, the _describe()_ method becomes handy as it computes various statstics for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical data can also be plotted in form of histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate reading, histograms can also be based on selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Year'].hist(bins=100) # bins determines the number of vertical bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex plots are also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# docs: http://matplotlib.org/api/text_api.html#matplotlib.text.Annotation\n",
    "#fig = plt.figure()\n",
    "df[['AbilitiesCount','PartnershipsCount','Year']].groupby(\"Year\").mean().plot(subplots=True)\n",
    "MaxValue = df['Year'].max()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Graph from our Data\n",
    "\n",
    "In the next step, we will create a graph from our data linking the superheroes with their partners and their abilities. Eventually, this will help us to discover which superheroes share abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAbilityPartnershipGraphs(consideredDataFrame):\n",
    "    # for testing purposes it is handy to limit the number of records\n",
    "    #consideredDataFrame=consideredDataFrame[(consideredDataFrame.Year>=1961) & (consideredDataFrame.Year<1975)]\n",
    "\n",
    "    # create an empty graph from the nx (networkx) package imported above\n",
    "    G=nx.Graph()\n",
    "    G_abilities=nx.Graph()\n",
    "    \n",
    "    # DEBUG dictionaries for ID output\n",
    "    heroIDs=dict()\n",
    "    heroID=0\n",
    "    abilityIDs=dict()\n",
    "    abilityID=0\n",
    "\n",
    "    # .itertuples() gives us an iterator over all rows in a data frame\n",
    "    for row in consideredDataFrame.itertuples():\n",
    "        hero=row[11]\n",
    "        year=row[2]\n",
    "        decade=row[3]\n",
    "        if not hero in heroIDs:\n",
    "            heroIDs[hero]=heroID\n",
    "            heroID=heroID+1\n",
    "            \n",
    "        if not hero in G.nodes():\n",
    "            # debug\n",
    "            #G.add_node(hero)\n",
    "            # √ºberall wo nun heroNr steht, stand hero\n",
    "            heroNr=heroIDs[hero]\n",
    "            G.add_node(heroIDs[hero])\n",
    "            # the name attribute will be helpful for D3.js visualizations (see below)\n",
    "            G.nodes[heroNr]['name'] = hero\n",
    "            G.nodes[heroNr]['year'] = year\n",
    "            G.nodes[heroNr]['decade'] = decade\n",
    "            G.nodes[heroNr]['group'] = 2 # will change the display color in D3.js and will be used if a text label has to be plotted for this node\n",
    "            G.nodes[heroNr]['picture']=row[9]\n",
    "            localFilePath=str(row[10])\n",
    "            if not localFilePath.upper()==\"NAN\":\n",
    "                G.nodes[heroNr]['localFilePath']=localFilePath\n",
    "            else:\n",
    "                G.nodes[heroNr]['localFilePath']=\"N/A\" # must not be left empty, otherwise corrupt JSON will be created below\n",
    "\n",
    "        if not hero in G_abilities.nodes():\n",
    "            # debug\n",
    "            #G_abilities.add_node(hero)\n",
    "            G_abilities.add_node(heroIDs[hero])\n",
    "            # √ºberall wo nun heroID steht, stand hero\n",
    "            heroNr=heroIDs[hero]\n",
    "            \n",
    "            G_abilities.nodes[heroNr]['name'] = hero\n",
    "            G_abilities.nodes[heroNr]['year'] = year\n",
    "            G_abilities.nodes[heroNr]['decade'] = decade\n",
    "            G_abilities.nodes[heroNr]['group'] = 2 # will change the display color in D3.js and will be used if a text label has to be plotted for this node\n",
    "            G_abilities.nodes[heroNr]['picture']=row[9]\n",
    "            localFilePath=str(row[10])\n",
    "            if not localFilePath.upper()==\"NAN\":\n",
    "                G_abilities.nodes[heroNr]['localFilePath']=localFilePath\n",
    "            else:\n",
    "                G_abilities.nodes[heroNr]['localFilePath']=\"N/A\" # must not be left empty, otherwise corrupt JSON will be created below\n",
    "\n",
    "        # treat partnerships  (PartnershipsCount)  \n",
    "        if row[7]: # a shortcut to find out whether a string is empty\n",
    "            if not row[12]==\"NAN\":\n",
    "                partners=row[12].split(\",\")\n",
    "                for partner in partners:\n",
    "                    if not partner in G.nodes():\n",
    "                        G.add_node(partner)\n",
    "                        G.nodes[partner]['name'] = partner\n",
    "                        G.nodes[partner]['year'] = year\n",
    "                        G.nodes[partner]['decade'] = decade\n",
    "                        G.nodes[partner]['group'] = 2\n",
    "                    G.add_edge(hero,partner)\n",
    "                    G[hero][partner]['year'] = year\n",
    "                    G[hero][partner]['decade'] = decade\n",
    "                    # this is a tricky part that is only needed for the later visualization of the results with D3.js\n",
    "                    # during visualization, we want to hide elements of the graph depending on the associated year.\n",
    "                    # however, ability nodes linked to different hero nodes (and thus years) must not be hidden.\n",
    "                    # to prevent this, we check for such nodes and give them a dummy year.\n",
    "                    edgeList=G.edges(partner,data=True)\n",
    "                    if len(edgeList)>1:\n",
    "                        for e in edgeList:\n",
    "                            G.nodes[e[0]]['year'] = \"XXX\"\n",
    "\n",
    "        # treat abilities\n",
    "        if row[5]: #AbilitiesCount\n",
    "            if not row[13]==\"NAN\":\n",
    "                abilities=str(row[13]).split(\",\")\n",
    "                for ab in abilities:\n",
    "                    if not ab in abilityIDs:\n",
    "                        abilityIDs[ab]=abilityID\n",
    "                        abilityID=abilityID+1\n",
    "                        \n",
    "                    if not ab in G_abilities.nodes():\n",
    "                        #debug\n",
    "                        #G_abilities.add_node(ab)\n",
    "                        G_abilities.add_node(abilityIDs[ab])\n",
    "                        # √ºberall wo nun abID steht, stand ab\n",
    "                        abID=abilityIDs[ab]\n",
    "                        \n",
    "                        G_abilities.nodes[abID]['name'] = ab\n",
    "                        G_abilities.nodes[abID]['year'] = year\n",
    "                        G_abilities.nodes[abID]['decade'] = decade\n",
    "                        G_abilities.nodes[abID]['group'] = 1 # will change the display color in D3.js\n",
    "                        #debug\n",
    "                     \n",
    "                    #debug \n",
    "                    #G_abilities.add_edge(hero,ab)\n",
    "                    #G_abilities[hero][ab]['year'] = year\n",
    "                    #G_abilities[hero][ab]['decade'] = decade\n",
    "                    G_abilities.add_edge(heroIDs[hero],abID)\n",
    "                    G_abilities[heroIDs[hero]][abID]['year'] = year\n",
    "                    G_abilities[heroIDs[hero]][abID]['decade'] = decade\n",
    "                    # this is a tricky part that is only needed for the later visualization of the results with D3.js\n",
    "                    # during visualization, we want to hide elements of the graph depending on the associated year.\n",
    "                    # however, ability nodes linked to different hero nodes (and thus years) must not be hidden.\n",
    "                    # to prevent this, we check for such nodes and give them a dummy year.\n",
    "                    #debug\n",
    "                    #edgeList=G_abilities.edges(ab,data=True)\n",
    "                    edgeList=G_abilities.edges(abID,data=True)\n",
    "                    if len(edgeList)>1:\n",
    "                        for e in edgeList:\n",
    "                            G_abilities.nodes[e[0]]['year'] = \"XXX\"\n",
    "                    print(\"Adding edge: \"+hero+\" - \"+str(ab))\n",
    "                #G_abilities.add_edges_from(list(itertools.product(abilities,abilities))) # macht ein cross product der abilities\n",
    "    return [G,G_abilities]\n",
    "\n",
    "# DEBUG\n",
    "#r=createAbilityPartnershipGraphs(df.head(30))\n",
    "r=createAbilityPartnershipGraphs(df)\n",
    "\n",
    "G=r[0]\n",
    "G_abilities=r[1]\n",
    "\n",
    "print(\"Statistics of the superhero partnership graph.\")\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "print(\"\\nStatistics of the superhero ability graph.\")\n",
    "print(G_abilities.number_of_nodes())\n",
    "print(G_abilities.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Data in Your Website\n",
    "\n",
    "If you want to display the data on an interactive website for a wider audience, Jupyter notebooks might no longer be appropriate. Fortunately, the networkx package allows us to export the graph data as JSON files which can be easily read by JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "# create needed subdirectories\n",
    "if not os.path.exists('./web/force/'):\n",
    "    os.mkdir('./web/force/')\n",
    "if not os.path.exists('./web/force/'+pathPrefix+\"/\"):\n",
    "    os.mkdir('./web/force/'+pathPrefix+\"/\")\n",
    "\n",
    "#d = json_graph.node_link_data(G)\n",
    "d = json_graph.node_link_data(G_abilities)\n",
    "\n",
    "    \n",
    "jsonPath='./web/force/'+pathPrefix+'/force.json'\n",
    "json.dump(d, open(jsonPath,'w'))\n",
    "\n",
    "printLog(\"Saved JSON file to: \"+jsonPath)\n",
    "# weitere notwendige Files f√ºr D3.js visualization unter: /Users/david/Documents/src/javascript/visualization\n",
    "# examples taken from https://github.com/networkx/networkx/tree/master/examples/javascript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As graphs can be become to large to be displayed responsively, we will also export the data separated by decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decades=[1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020]\n",
    "dataframes=OrderedDict()\n",
    "for decade in decades:\n",
    "    dfTemp=df[(df.Decade==decade)]\n",
    "    dataframes[decade]=dfTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Decade==2020].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedStats=df[['AbilitiesCount','PartnershipsCount',\"Decade\"]].groupby(\"Decade\").count()\n",
    "groupedStats.to_csv(\"./web/force/\"+pathPrefix+\"/decadeStats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for decade in dataframes:\n",
    "    currentDF=dataframes[decade]\n",
    "    r=createAbilityPartnershipGraphs(currentDF)\n",
    "    d = json_graph.node_link_data(r[1])\n",
    "    jsonPath='./web/force/'+pathPrefix+\"/\"+str(decade)+'.json'\n",
    "    json.dump(d, open(jsonPath,'w'))\n",
    "\n",
    "    printLog(\"Saved JSON file to: \"+jsonPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the Results\n",
    "\n",
    "Now we are set to display the results in a browser. However, to prevent security warnings, we cannot directly open the HTML files saved in the web/ directory. Hence, we launch a built-in Python webserver for display.\n",
    "\n",
    "_ATTENTION_ Never interrupt the kernel after you have started the following cell. Just press <ENTER> in the input field displayed in the cell below.\n",
    "\n",
    "If you have accidently interrupted the kernel, Python will not release the listener on port 8000. Thus, you will not be able to re-run the following cell. To solve the problem, you must restart the whole Jupyter notebook process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you make changes to http_server.py you will have to delete http_server.pyc manually\n",
    "import http_server\n",
    "if isMarvel:\n",
    "    http_server.load_url('./web/force/force_marvel.html')\n",
    "else:\n",
    "    http_server.load_url('./web/force/force_dc.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
