{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The %... is an iPython thing, and is not part of the Python language.\n",
    "# In this case we're just telling the plotting library to draw things on\n",
    "# the notebook, instead of on a separate window.\n",
    "%matplotlib inline\n",
    "\n",
    "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
    "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict # provides the ordered dictionary\n",
    "import re # for regular expressions used below\n",
    "import urllib # to read from URLs\n",
    "import json\n",
    "import networkx as nx # network analysis\n",
    "from networkx.readwrite import json_graph\n",
    "import itertools\n",
    "import os.path\n",
    "from datetime import datetime # for time measurement\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import subprocess as subp\n",
    "import gzip\n",
    "import math\n",
    "import codecs\n",
    "\n",
    "from jellyfish import jaro_distance, jaro_winkler, hamming_distance, levenshtein_distance\n",
    "import scipy.cluster.hierarchy as scipycluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from skimage import io, exposure\n",
    "from scipy.spatial import distance\n",
    "# import the k-means algorithm\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin,pairwise_distances_argmin_min, pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "\n",
    "# image handling\n",
    "from PIL import Image\n",
    "\n",
    "# geo stuff\n",
    "from geopy.distance import vincenty\n",
    "import geojson as gj\n",
    "\n",
    "import csv\n",
    "\n",
    "def printLog(text):\n",
    "    now=str(datetime.now())\n",
    "    print(\"[\"+now+\"]\\t\"+text)\n",
    "    # forces to output the result of the print command immediately, see: http://stackoverflow.com/questions/230751/how-to-flush-output-of-python-print\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def pickleCompress(fileName,pickledObject):\n",
    "    printLog(\"Pickling to '%s'\" %fileName)\n",
    "    f = gzip.open(fileName,'wb')\n",
    "    pickle.dump(pickledObject,f)\n",
    "    f.close()\n",
    "    printLog(\"Pickling done.\")\n",
    "    \n",
    "def pickleDecompress(fileName):\n",
    "    #restore the object\n",
    "    printLog(\"Depickling from '%s'\" %fileName)\n",
    "    f = gzip.open(fileName,'rb')\n",
    "    pickledObject = pickle.load(f)\n",
    "    f.close()\n",
    "    printLog(\"Depickling done.\")\n",
    "    return pickledObject\n",
    "\n",
    "if not os.path.exists(\"./analysis/\"):\n",
    "        os.makedirs(\"./analysis/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph(data):\n",
    "    # create an empty graph from the nx (networkx) package imported above\n",
    "    G=nx.DiGraph()\n",
    "\n",
    "    rowCount=0\n",
    "    seenAuthors=[]\n",
    "    seenLocations=[]\n",
    "    seenPublishers=[]\n",
    "    #noRecords=len(data.keys())\n",
    "    #print(\"Processing %i records.\"%noRecords)\n",
    "    for ppn in data:\n",
    "        #print(ppn)\n",
    "        rowCount=rowCount+1\n",
    "        #if rowCount%10000==0:\n",
    "        #    printLog(\"Processed %i records of %i\"%(rowCount,noRecords))\n",
    "        \n",
    "        \n",
    "        author=data[ppn][\"author\"]\n",
    "        authorID=data[ppn][\"authorID\"]\n",
    "        publisher=data[ppn][\"publisher\"]\n",
    "        publisherLocation=data[ppn][\"publisherLocation\"]\n",
    "        \n",
    "        title=data[ppn][\"title\"]\n",
    "        \n",
    "        if author:\n",
    "            if not author in seenAuthors:\n",
    "                seenAuthors.append(author)\n",
    "                G.add_node(author)\n",
    "                # the name attribute will be helpful for D3.js visualizations\n",
    "                G.node[author]['name'] = author\n",
    "                G.node[author]['type'] = \"author\"\n",
    "                if authorID:\n",
    "                    G.node[author]['id'] = authorID\n",
    "        if publisher:\n",
    "            if not publisher in seenPublishers:\n",
    "                seenPublishers.append(publisher)\n",
    "                G.add_node(publisher)\n",
    "                G.node[publisher]['name'] = publisher\n",
    "                G.node[publisher]['type'] = \"publisher\"\n",
    "       \n",
    "        if publisherLocation:\n",
    "            if not publisherLocation in seenLocations:\n",
    "                seenLocations.append(publisherLocation)\n",
    "                G.add_node(publisherLocation)\n",
    "                G.node[publisherLocation]['name'] = publisherLocation\n",
    "                G.node[publisherLocation]['type'] = \"publisherLocation\"\n",
    "        \n",
    "        if title:\n",
    "            G.add_node(title)\n",
    "            G.node[title]['name'] = title\n",
    "            G.node[title]['type'] = \"title\"\n",
    "        \n",
    "        if author and title:\n",
    "            G.add_edge(author,title)\n",
    "        if publisher and title:\n",
    "            G.add_edge(publisher,title)\n",
    "            \n",
    "        if author and publisher:\n",
    "            G.add_edge(author,publisher)\n",
    "        if publisher and publisherLocation:\n",
    "            G.add_edge(publisher,publisherLocation)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph_Author_Publisher_Location(data):\n",
    "    # create an empty graph from the nx (networkx) package imported above\n",
    "    G=nx.DiGraph()\n",
    "\n",
    "    rowCount=0\n",
    "    seenAuthors=[]\n",
    "    seenLocations=[]\n",
    "    seenPublishers=[]\n",
    "    #noRecords=len(data.keys())\n",
    "    #print(\"Processing %i records.\"%noRecords)\n",
    "    for ppn in data:\n",
    "        #print(ppn)\n",
    "        rowCount=rowCount+1\n",
    "        #if rowCount%10000==0:\n",
    "        #    printLog(\"Processed %i records of %i\"%(rowCount,noRecords))\n",
    "        \n",
    "        author=data[ppn][\"author\"]\n",
    "        authorID=data[ppn][\"authorID\"]\n",
    "        publisher=data[ppn][\"publisher\"]\n",
    "        publisherLocation=data[ppn][\"publisherLocation\"]\n",
    "        \n",
    "        title=data[ppn][\"title\"]\n",
    "        \n",
    "        if author:\n",
    "            if not author in seenAuthors:\n",
    "                seenAuthors.append(author)\n",
    "                G.add_node(author)\n",
    "                # the name attribute will be helpful for D3.js visualizations\n",
    "                G.node[author]['name'] = author\n",
    "                G.node[author]['type'] = \"author\"\n",
    "                if authorID:\n",
    "                    G.node[author]['id'] = authorID\n",
    "        if publisher:\n",
    "            if not publisher in seenPublishers:\n",
    "                seenPublishers.append(publisher)\n",
    "                G.add_node(publisher)\n",
    "                G.node[publisher]['name'] = publisher\n",
    "                G.node[publisher]['type'] = \"publisher\"\n",
    "       \n",
    "        if publisherLocation:\n",
    "            if not publisherLocation in seenLocations:\n",
    "                seenLocations.append(publisherLocation)\n",
    "                G.add_node(publisherLocation)\n",
    "                G.node[publisherLocation]['name'] = publisherLocation\n",
    "                G.node[publisherLocation]['type'] = \"publisherLocation\"\n",
    "            \n",
    "        if author and publisher:\n",
    "            G.add_edge(author,publisher)\n",
    "        if publisher and publisherLocation:\n",
    "            G.add_edge(publisher,publisherLocation)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://networkx.github.io/documentation/stable/reference/readwrite/json_graph.html JSON output is compatible with d3.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-03-04 17:23:55.834287]\tProcessing catalog language: fry\n",
      "[2019-03-04 17:23:55.834911]\tCreating graph...\n",
      "[2019-03-04 17:23:55.894831]\tProcessing catalog language: fry\n",
      "[2019-03-04 17:23:55.895478]\tCreating graph...\n",
      "[2019-03-04 17:23:55.924303]\tProcessing catalog language: ice\n",
      "[2019-03-04 17:23:55.924814]\tCreating graph...\n",
      "[2019-03-04 17:23:56.071901]\tProcessing catalog language: ice\n",
      "[2019-03-04 17:23:56.072512]\tCreating graph...\n",
      "[2019-03-04 17:23:56.187140]\tProcessing catalog language: por\n",
      "[2019-03-04 17:23:56.187641]\tCreating graph...\n",
      "[2019-03-04 17:23:57.352159]\tProcessing catalog language: por\n",
      "[2019-03-04 17:23:57.352663]\tCreating graph...\n",
      "[2019-03-04 17:23:57.971977]\tProcessing catalog language: nor\n",
      "[2019-03-04 17:23:57.972766]\tCreating graph...\n",
      "[2019-03-04 17:24:00.885687]\tProcessing catalog language: nor\n",
      "[2019-03-04 17:24:00.886498]\tCreating graph...\n",
      "[2019-03-04 17:24:02.523848]\tProcessing catalog language: dan\n",
      "[2019-03-04 17:24:02.524672]\tCreating graph...\n",
      "[2019-03-04 17:24:09.143996]\tProcessing catalog language: dan\n",
      "[2019-03-04 17:24:09.145193]\tCreating graph...\n",
      "[2019-03-04 17:24:13.463392]\tProcessing catalog language: swe\n",
      "[2019-03-04 17:24:13.463938]\tCreating graph...\n",
      "[2019-03-04 17:24:32.728642]\tProcessing catalog language: swe\n",
      "[2019-03-04 17:24:32.729588]\tCreating graph...\n",
      "[2019-03-04 17:24:45.526091]\tProcessing catalog language: spa\n",
      "[2019-03-04 17:24:45.526558]\tCreating graph...\n",
      "[2019-03-04 17:25:30.156874]\tProcessing catalog language: spa\n",
      "[2019-03-04 17:25:30.157969]\tCreating graph...\n",
      "[2019-03-04 17:26:07.850611]\tProcessing catalog language: dut\n",
      "[2019-03-04 17:26:07.851714]\tCreating graph...\n",
      "[2019-03-04 17:28:29.444823]\tProcessing catalog language: dut\n",
      "[2019-03-04 17:28:29.445516]\tCreating graph...\n",
      "[2019-03-04 17:30:38.831615]\tProcessing catalog language: ita\n",
      "[2019-03-04 17:30:38.832140]\tCreating graph...\n",
      "[2019-03-04 17:40:10.664123]\tProcessing catalog language: ita\n",
      "[2019-03-04 17:40:10.665328]\tCreating graph...\n",
      "[2019-03-04 17:49:24.805996]\tProcessing catalog language: lat\n",
      "[2019-03-04 17:49:24.806532]\tCreating graph...\n",
      "[2019-03-04 18:20:26.095861]\tProcessing catalog language: lat\n",
      "[2019-03-04 18:20:26.096490]\tCreating graph...\n",
      "[2019-03-04 18:49:52.781365]\tProcessing catalog language: fre\n",
      "[2019-03-04 18:49:52.781910]\tCreating graph...\n",
      "[2019-03-04 20:19:43.121489]\tProcessing catalog language: fre\n",
      "[2019-03-04 20:19:43.122470]\tCreating graph...\n",
      "[2019-03-04 21:40:31.150803]\tProcessing catalog language: eng\n",
      "[2019-03-04 21:40:31.151407]\tCreating graph...\n",
      "[2019-03-05 04:45:44.663410]\tProcessing catalog language: eng\n",
      "[2019-03-05 04:45:44.665033]\tCreating graph...\n",
      "[2019-03-05 11:48:44.980834]\tProcessing catalog language: ger\n",
      "[2019-03-05 11:48:45.009399]\tCreating graph...\n"
     ]
    }
   ],
   "source": [
    "baseDir=\"/Users/david/src/__datasets/cbs_analysis/\"\n",
    "# the following catalog files are sorted by file size\n",
    "files=[\"fry_out.txt\",\"ice_out.txt\",\"por_out.txt\",\"nor_out.txt\",\"dan_out.txt\",\"swe_out.txt\",\"spa_out.txt\",\"dut_out.txt\",\"ita_out.txt\",\"lat_out.txt\",\"fre_out.txt\",\"eng_out.txt\",\"ger_out.txt\"]\n",
    "\n",
    "# create a dictionary for the records\n",
    "records=dict()\n",
    "\n",
    "dataFrameDict={\"language\":[],\"graph_type\":[],\"nodes\":[],\"edges\":[],\"creation_duration\":[],\"records\":[]}\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "for file in files:\n",
    "    currentLanguage=file.split(\"_\")[0]\n",
    "    ppn=None\n",
    "    with open(baseDir+file, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in spamreader:\n",
    "            # skip empty separator line\n",
    "            if row:\n",
    "                # if we haven't seen a PPN before, it is a new record we have to deal with\n",
    "                if not ppn:\n",
    "                    ppn=row[0]\n",
    "                    # an empty dict for the record values\n",
    "                    values={\"title\":None,\"author\":None,\"authorID\":None,\"publisher\":None,\"publisherLocation\":None}\n",
    "                    records[ppn]=values\n",
    "                # check if we deal with a row containing a title\n",
    "                if row[1]==\"021A\":\n",
    "                    records[ppn][\"title\"]=row[2]\n",
    "                # publisher\n",
    "                if row[1]==\"033A\":\n",
    "                    tokens=row[2].split(\"@\")\n",
    "                    if len(tokens)>=2:\n",
    "                        records[ppn][\"publisher\"]=tokens[0]\n",
    "                        records[ppn][\"publisherLocation\"]=tokens[1].replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                    else:\n",
    "                        records[ppn][\"publisher\"]=row[2]\n",
    "                # author\n",
    "                if row[1]==\"028A\":\n",
    "                    tokens=row[2].split(\"@\")\n",
    "                    if len(tokens)>=2:\n",
    "                        records[ppn][\"author\"]=tokens[0]\n",
    "                        records[ppn][\"authorID\"]=tokens[1]\n",
    "                    else:\n",
    "                        records[ppn][\"author\"]=row[2]             \n",
    "            # in case of an empty line, prepare for a new record\n",
    "            else:\n",
    "                #debug\n",
    "                #if ppn:\n",
    "                #    print(ppn+str(records[ppn]))\n",
    "                ppn=None\n",
    "    # * * * * * * * * * * * * * * *\n",
    "    # process the found records\n",
    "    # * * * * * * * * * * * * * * *\n",
    "    noRecords=len(records.keys())\n",
    "    \n",
    "    dataFrameDict[\"language\"].append(currentLanguage)\n",
    "    dataFrameDict[\"graph_type\"].append(\"author_publisher_location_title\")\n",
    "    printLog(\"Processing catalog language: %s\"%currentLanguage)\n",
    "    printLog(\"Creating graph...\")\n",
    "    startTime = datetime.now()\n",
    "    returnedGraph=createGraph(records)\n",
    "    endTime = datetime.now()\n",
    "    dataFrameDict[\"nodes\"].append(len(returnedGraph.nodes()))\n",
    "    dataFrameDict[\"edges\"].append(len(returnedGraph.edges()))\n",
    "    dataFrameDict[\"records\"].append(noRecords)\n",
    "    dataFrameDict[\"creation_duration\"]=endTime-startTime\n",
    "    #printLog(\"Serializing graph with %i nodes and %i edges.\"%(len(returnedGraph.nodes()),len(returnedGraph.edges())))\n",
    "    nx.write_gml(returnedGraph,\"analysis/\"+currentLanguage+\"_author_publisher_location_title.gml\")\n",
    "    jsonData = json_graph.node_link_data(returnedGraph, {'link': 'edges', 'source': 'from', 'target': 'to'})\n",
    "    with open(\"analysis/\"+currentLanguage+\"_author_publisher_location_title.json\", \"w\") as write_file:\n",
    "        json.dump(jsonData, write_file,default={'link': 'edges', 'source': 'from', 'target': 'to'})\n",
    "    nx.write_graphml_lxml(returnedGraph, \"analysis/\"+currentLanguage+\"_author_publisher_location_title.graphml\")\n",
    "    \n",
    "    dataFrameDict[\"language\"].append(currentLanguage)\n",
    "    dataFrameDict[\"graph_type\"].append(\"author_publisher_location_title\")\n",
    "    printLog(\"Processing catalog language: %s\"%currentLanguage)\n",
    "    printLog(\"Creating graph...\")\n",
    "    startTime = datetime.now()\n",
    "    returnedGraph=createGraph_Author_Publisher_Location(records)\n",
    "    endTime = datetime.now()\n",
    "    dataFrameDict[\"nodes\"].append(len(returnedGraph.nodes()))\n",
    "    dataFrameDict[\"edges\"].append(len(returnedGraph.edges()))\n",
    "    dataFrameDict[\"records\"].append(noRecords)\n",
    "    dataFrameDict[\"creation_duration\"]=endTime-startTime\n",
    "    #printLog(\"Serializing graph with %i nodes and %i edges.\"%(len(returnedGraph.nodes()),len(returnedGraph.edges())))\n",
    "    nx.write_gml(returnedGraph,\"analysis/\"+currentLanguage+\"_author_publisher_location.gml\")\n",
    "    jsonData = json_graph.node_link_data(returnedGraph, {'link': 'edges', 'source': 'from', 'target': 'to'})\n",
    "    with open(\"analysis/\"+currentLanguage+\"_author_publisher_location.json\", \"w\") as write_file:\n",
    "        json.dump(jsonData, write_file,default={'link': 'edges', 'source': 'from', 'target': 'to'})\n",
    "    nx.write_graphml_lxml(returnedGraph, \"analysis/\"+currentLanguage+\"_author_publisher_location.graphml\")\n",
    "    \n",
    "printLog(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataFrameDict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deutschsprachiger Graph:\n",
    "[2019-03-03 21:10:04.826571]\tCreating graph...\n",
    "[2019-03-04 00:49:56.174232]\tDone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://networkx.github.io/documentation/stable/reference/algorithms/approximation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-03-04 15:12:35.226562]\tComputing graph properties...\n",
      "[2019-03-04 15:12:35.238867]\tDone.\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import centrality\n",
    "printLog(\"Computing graph properties...\")\n",
    "degreeCentralities=centrality.degree_centrality(returnedGraph)\n",
    "inDegrees=centrality.in_degree_centrality(returnedGraph)\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-03-04 15:09:25.855886]\tComputing graph properties...\n",
      "[2019-03-04 15:09:25.856747]\tDone.\n"
     ]
    }
   ],
   "source": [
    "# very sloooooooow...\n",
    "from networkx.algorithms import approximation\n",
    "printLog(\"Computing graph properties...\")\n",
    "#s=approximation.max_clique(returnedGraph)\n",
    "printLog(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
